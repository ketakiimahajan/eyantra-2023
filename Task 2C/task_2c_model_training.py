""" 
    Eyantra / Stage 1 / Task 2B
    Team : GG_1880
    Model to classify images into 5 classes using tensorflow

"""

# -*- coding: utf-8 -*-
"""task2B_tf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z4LvDTYjAaN2N8xgY6_mMa1ekChTYHJZ

### **Installing dependencies, removing dodgy images, loading data**
"""

# Importing necessary libraries
import tensorflow as tf
import os
import cv2
import imghdr
import numpy as np
from matplotlib import pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten
from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy
from tensorflow.keras.models import load_model

# Avoid OOM errors by setting GPU Memory Consumption Growth
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

# Path to training data
train_path = '/content/drive/MyDrive/sy/training/training'

# List of image extensions
image_exts = ['jpeg','jpg', 'bmp', 'png']

# Removing invalid images
for image_class in os.listdir(train_path):
    for image in os.listdir(os.path.join(train_path, image_class)):
        image_path = os.path.join(train_path, image_class, image)
        try:
            img = cv2.imread(image_path)
            tip = imghdr.what(image_path)
            if tip not in image_exts:
                print('Image not in ext list {}'.format(image_path))
                os.remove(image_path)
        except Exception as e:
            print('Issue with image {}'.format(image_path))
            # os.remove(image_path)

# Loading image data using TensorFlow
data = tf.keras.utils.image_dataset_from_directory('/content/drive/MyDrive/sy/training/training')

# Creating an iterator for data processing
data_iterator = data.as_numpy_iterator()

# Displaying a batch of data
batch = data_iterator.next()
# batch
# len(batch)

# Plotting images from the batch
fig, ax = plt.subplots(ncols=4, figsize=(20,20))
for idx, img in enumerate(batch[0][:4]):
    ax[idx].imshow(img.astype(int))
    ax[idx].title.set_text(batch[1][idx])


"""### **Scaling Data**"""

# Scaling the image data
data = data.map(lambda x,y: (x/255, y))

# Creating an iterator for scaled data
scaled_iterator = data.as_numpy_iterator()

# Fetching the next batch from scaled data
batch = scaled_iterator.next()

# scaled_iterator.next()[0].max()


"""### **Split Data**"""

# Splitting the dataset into training, validation, and test sets
len(data)

# Creating training, validation, and test sets
train_size = int(len(data)*.7)
val_size = int(len(data)*.2)
test_size = int(len(data)*.1)

# test_size

train = data.take(train_size)
val = data.skip(train_size).take(val_size)
test = data.skip(train_size+val_size).take(test_size)


"""### **Deep Learning Model**"""


# Creating a sequential deep learning model
model = Sequential()

# Adding convolutional and pooling layers
model.add(Conv2D(16, (3,3), 1, activation='relu', input_shape=(256,256,3)))
model.add(MaxPooling2D())

model.add(Conv2D(32, (3,3), 1, activation='relu'))
model.add(MaxPooling2D())

model.add(Conv2D(16, (3,3), 1, activation='relu'))
model.add(MaxPooling2D())

model.add(Flatten())

# Adding dense layers
model.add(Dense(256, activation='relu'))
model.add(Dense(5, activation='softmax'))

# Compiling the model
model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Displaying the model summary
model.summary()


"""### **Training**"""


# Setting up callbacks for training
logdir='/content/drive/MyDrive/sy'

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)

# Training the model
hist = model.fit(train, epochs=20, validation_data=val, callbacks=[tensorboard_callback])

# Displaying the history of the training process
hist.history


"""### **Plotting Perfomance**"""


# Plotting the performance metrics
fig = plt.figure()
plt.plot(hist.history['loss'], color='teal', label='loss')
plt.plot(hist.history['val_loss'], color='orange', label='val_loss')
fig.suptitle('Loss', fontsize=20)
plt.legend(loc="upper left")
plt.show()

fig = plt.figure()
plt.plot(hist.history['accuracy'], color='teal', label='accuracy')
plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')
fig.suptitle('Accuracy', fontsize=20)
plt.legend(loc="upper left")
plt.show()


"""### **Evaluating Performance**"""


# precision = Precision()
# recall = Recall()
# bin_accuracy = BinaryAccuracy()

# for batch in test.as_numpy_iterator():
#     X, y = batch
#     yhat = model.predict(X)
#     precision.update_state(y, yhat)
#     recall.update_state(y, yhat)
#     bin_accuracy.update_state(y, yhat)

# print(precision.result().numpy(), recall.result().numpy(), bin_accuracy.result().numpy())


"""### **Testing New Images**"""


# Loading and displaying a new image
img = cv2.imread('/content/drive/MyDrive/sy/testing/testing/building1.jpeg')
# /content/drive/MyDrive/sy/testing/testing/combat1.jpeg
# /content/drive/MyDrive/sy/testing/testing/fire1.jpeg
# /content/drive/MyDrive/sy/testing/testing/military1.jpeg
# /content/drive/MyDrive/sy/testing/testing/rehab1.jpeg

plt.imshow(img)
plt.show()

# Resizing the image for model testing
resize = tf.image.resize(img, (256,256))
plt.imshow(resize.numpy().astype(int))
plt.show()

# Making predictions on the resized image
yhat = model.predict(np.expand_dims(resize/255, 0))

yhat


"""### **Saving Model**"""


model.save(os.path.join('/content/drive/MyDrive/sy','imageclassifier.h5'))

new_model = load_model(os.path.join('/content/drive/MyDrive/sy','imageclassifier.h5'))

new_model.predict(np.expand_dims(resize/255, 0))